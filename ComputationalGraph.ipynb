{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2:  Computational Graph\n",
    "\n",
    "\n",
    "\n",
    "During last TP we asked you to make the backward pass, implementing all the derivatives needed. \n",
    "As you can expect, doing this every time you make a new model is a little redundant. \n",
    "The ML libraries allow you to implement models by focusing only on the forward pass, they construct than a graph and compute derivatives from the bottom to the leafs of the graph. This graph is known as \"computational graph\".\n",
    "\n",
    "The aim of this TP is to build a computational graph inspired by [pytorch](https://pytorch.org/), and than test it with a simple model (MLP).\n",
    "The construction of the model, of the loss and the optimizer are also inspired by pytorch.\n",
    "\n",
    "The transition to pytorch should be easy in the future.\n",
    "\n",
    "**Disclaimer** This code is inspired by how users use Pytorch, it doesn't replace Pytorch and the implemtatin differ form Pytorch. The only goal of this TP is to give an intuition how pytorch and the computational graph work, before starting using it!\n",
    "\n",
    "## The Computational Graph\n",
    "\n",
    "The computational graph is a graph that specifies the operations done to get a given value. \n",
    "If C = A + B, the graph will look like:\n",
    "```\n",
    "A\n",
    " \\\n",
    "  \\\n",
    "   + -- C \n",
    "  /\n",
    " /\n",
    "B\n",
    "```\n",
    "or if F = C * D and E = log(F), the graph will look like:\n",
    "```\n",
    "A\n",
    " \\\n",
    "  \\\n",
    "   + -- C\n",
    "  /      \\\n",
    " /        \\\n",
    "B          * -- F -- log -- E\n",
    "          /\n",
    "         /\n",
    "        D\n",
    "```\n",
    "\n",
    "As you can see, the graph is build during the \"forward pass\" and it is easy to see the gradients flow (start on E and end on the leafs A, B and D. \n",
    "\n",
    "### Variable\n",
    "To build this graph in our code, we introduce an object called Variable. This object will look like an array in numpy, it will content data, and have methods like mean, sum, t, etc. Variable has also a grad, a grad__fn, and children field.\n",
    "\n",
    "* Variable.grad: store the gradients during the backward pass for the given variable (same shape as Variabale.shape)\n",
    "* Variable.grad_fn: store the function that has built this Variable (addition, multiplication, etc)\n",
    "* Variable.children: list of all the operations where the Variable was used. We need Variable.children during backward pass to know if all the children have propagate thier gradients before the current variable compute in turn its gradients.\n",
    "\n",
    "### Functions\n",
    "\n",
    "Functions cointains all the oprations we can use in your code. Each operation needs a forward and a backward method.\n",
    "The forward method is simply the __init__ method where you compute and store the result of the operation. \n",
    "There is 2 backward methods:\n",
    "* backward (general): Inherited from the _Function parent class, it calls the second _backward method (see below) and updates the gradients of the variables used to build the current one.\n",
    "* _backward (specific): Is specific to each operation, computes the gradients for its parents. The derivatives are computed according to the specific operation.\n",
    "\n",
    "### Functional\n",
    "\n",
    "Is simply an interface for all functions defined in functions.\n",
    "You don't need to touch this file. but take a look at it, because it gives you all the functions you have add.\n",
    "If you don't use a function from this interface, you will not be able to construct the graph and ptopagate trough it.\n",
    "\n",
    "*|!\\* Even the standart operation you can use directly: +, -, *, / use the operation of functional! Take a look at Variable.__add__, Variable.__sub__, Variable.__mul__, Variable.__truediv__ if you doubt.\n",
    "\n",
    "\n",
    "## What we ask you to do:\n",
    "\n",
    "Complete the *Fill here* in the following cells of this notebook.\n",
    "Use latex notation to add your formulas. \n",
    "Once you have filled the missing parts of an operation, go to function.py and implement the missing parts of it. Tests are provided by saving the gradients using pytorch on the same conditions (same arrays, gradients cleared between operations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before starting with the derivatives, let's take a look at variable.py.\n",
    "The majority of this class is provided to you. We **ask you to describe a little bit this class in your report, \n",
    "mostly the methods backward and update_grad**. Than fill in the missing parts in this two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the Variables you will use to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Informations:\n",
      "\n",
      "Name: a\n",
      "Data:\n",
      " [[4.5]]\n",
      "Shape: (1, 1)\n",
      "Grad:\n",
      " None\n",
      "Grad_fn: None\n"
     ]
    }
   ],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from functional import F\n",
    "from variable import Variable\n",
    "\n",
    "def display_variable_information(name, var):\n",
    "    print(\"\\nName:\", name)\n",
    "    print(\"Data:\\n\", var.data)\n",
    "    print(\"Shape:\", var.shape)\n",
    "    print(\"Grad:\\n\", var.grad)\n",
    "    print(\"Grad_fn:\", var.grad_fn)\n",
    "\n",
    "# scalars\n",
    "a = Variable([4.5])\n",
    "b = Variable([6.78])\n",
    "\n",
    "# arrays\n",
    "C = Variable([[1.73, 2.83], [5.13, 8.43], [5.13, 8.43]])\n",
    "D = Variable([[3.57, 4.96], [2.06, 1.94], [5.13, 8.43]])\n",
    "\n",
    "\n",
    "print(\"Variables Informations:\")\n",
    "# uncomment if you want\n",
    "display_variable_information(\"a\", a)\n",
    "#display_variable_information(\"b\", b)\n",
    "#display_variable_information(\"c\", C)\n",
    "#display_variable_information(\"d\", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable([[10.    2.83],\n",
      "          [ 5.13  8.43],\n",
      "          [ 5.13  8.43]])\n",
      "Variable([[2.83]])\n",
      "Variable([[10.    2.83],\n",
      "          [ 5.13  8.43],\n",
      "          [ 5.13  8.43]])\n",
      "None\n",
      "Variable([[39.95]])\n"
     ]
    }
   ],
   "source": [
    "print(C)\n",
    "print(C[0,1])\n",
    "\n",
    "C[0,0] = 10\n",
    "\n",
    "print(C)\n",
    "\n",
    "m = C + D\n",
    "m = F.add(C, D)\n",
    "\n",
    "\n",
    "k = C.sum()\n",
    "print(k.grad_fn)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from check_values import check_result_and_grads\n",
    "\n",
    "def clear_variables(*argv):\n",
    "    \"\"\"Clear all Variables passed in arguments.\"\"\"\n",
    "    for var in argv:\n",
    "        var.grad = None\n",
    "        var.grad_fn = None\n",
    "        var.children = []\n",
    "        var.retained_values = {}\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition\n",
    "\n",
    "**Given to you as example** \n",
    "\n",
    "**Inputs**: $x, y$\n",
    "\n",
    "**Operation**: $f(x,y) = x + y$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = 1$\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot 1 = \\frac{\\partial}{\\partial f}$\n",
    "\n",
    "+ **w.r.t.** $y$:\n",
    "\n",
    "    $\\frac{\\partial f}{\\partial y} = 1$\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial y} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial f} \\cdot 1 = \\frac{\\partial}{\\partial f}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = a + b\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, b, operation=\"addition\", itype=\"scalar\")\n",
    "\n",
    "res_array = C + D\n",
    "res_array.mean().backward()\n",
    "check_result_and_grads(res_array, C, C, operation=\"addition\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtraction\n",
    "\n",
    "**Inputs**: $x, y$\n",
    "\n",
    "**Operation**: $f(x,y) = x - y$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "\n",
    "+ **w.r.t.** $y$:\n",
    "\n",
    "    $\\frac{\\partial f}{\\partial y} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial y} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial y} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = a - b\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, b, operation=\"subtraction\", itype=\"scalar\")\n",
    "\n",
    "res_array = C - D\n",
    "res_array.mean().backward()\n",
    "check_result_and_grads(res_array, C, D, operation=\"subtraction\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplication\n",
    "\n",
    "**Inputs**: $x, y$\n",
    "\n",
    "**Operation**: $f(x,y) = x * y$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "\n",
    "+ **w.r.t.** $y$:\n",
    "\n",
    "    $\\frac{\\partial f}{\\partial y} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial y} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial y} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = a * b\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, b, operation=\"multiplication\", itype=\"scalar\")\n",
    "\n",
    "res_array = C * D\n",
    "res_array.mean().backward()\n",
    "check_result_and_grads(res_array, C, D, operation=\"multiplication\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Division\n",
    "\n",
    "**Inputs**: $x, y$\n",
    "\n",
    "**Operation**: $f(x,y) = x / y$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "\n",
    "+ **w.r.t.** $y$:\n",
    "\n",
    "    $\\frac{\\partial f}{\\partial y} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial y} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial y} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = a / b\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, b, operation=\"division\", itype=\"scalar\")\n",
    "\n",
    "res_array = C / D\n",
    "res_array.mean().backward()\n",
    "check_result_and_grads(res_array, C, D, operation=\"division\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication\n",
    "\n",
    "**Inputs**: $x, y$\n",
    "\n",
    "**Operation**: $f(x,y) = x.dot(y)$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "\n",
    "+ **w.r.t.** $y$:\n",
    "\n",
    "    $\\frac{\\partial f}{\\partial y} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial y} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial y} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_array = F.matmul(C.t(), D)\n",
    "res_array.mean().backward()\n",
    "\n",
    "check_result_and_grads(res_array, C, D, operation=\"matMul\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential\n",
    "\n",
    "**Inputs**: $x$\n",
    "\n",
    "**Operation**: $f(x) = e^x$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = F.exp(a)\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, operation=\"exp\", itype=\"scalar\")\n",
    "\n",
    "res_array = F.exp(C)\n",
    "res_array.mean().backward()\n",
    "check_result_and_grads(res_array, C, operation=\"exp\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Logarithm\n",
    "\n",
    "**Inputs**: $x$\n",
    "\n",
    "**Operation**: $f(x) = ln(x)$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = F.log(a)\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, operation=\"log\", itype=\"scalar\")\n",
    "\n",
    "res_array = F.log(C)\n",
    "res_array.mean().backward()\n",
    "check_result_and_grads(res_array, C, operation=\"log\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinus\n",
    "\n",
    "**Inputs**: $x$\n",
    "\n",
    "**Operation**: $f(x) = \\sin(x)$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = F.sin(a)\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, operation=\"sin\", itype=\"scalar\")\n",
    "\n",
    "res_array = F.sin(C)\n",
    "res_array.mean().backward()\n",
    "check_result_and_grads(res_array, C, operation=\"sin\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosinus\n",
    "\n",
    "**Inputs**: $x$\n",
    "\n",
    "**Operation**: $f(x) = \\cos(x)$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = F.cos(a)\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, operation=\"cos\", itype=\"scalar\")\n",
    "\n",
    "res_array = F.cos(C)\n",
    "res_array.mean().backward()\n",
    "check_result_and_grads(res_array, C, operation=\"cos\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tangent\n",
    "\n",
    "**Inputs**: $x$\n",
    "\n",
    "**Operation**: $f(x) = \\tan(x)$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = F.tan(a)\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, operation=\"tan\", itype=\"scalar\")\n",
    "\n",
    "res_array = F.tan(C)\n",
    "res_array.mean().backward()\n",
    "check_result_and_grads(res_array, C, operation=\"tan\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "\n",
    "**Inputs**: $x$\n",
    "\n",
    "**Operation**: $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = F.sigmoid(a)\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, operation=\"sigmoid\", itype=\"scalar\")\n",
    "\n",
    "res_array = F.sigmoid(C)\n",
    "res_array[0,0].backward()\n",
    "check_result_and_grads(res_array, C, operation=\"sigmoid\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh\n",
    "\n",
    "**Inputs**: $x$\n",
    "\n",
    "**Operation**: $f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = F.tanh(a)\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, operation=\"tanh\", itype=\"scalar\")\n",
    "\n",
    "res_array = F.tanh(C)\n",
    "res_array[0,0].backward()\n",
    "check_result_and_grads(res_array, C, operation=\"tanh\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLu\n",
    "\n",
    "**Inputs**: $x$\n",
    "\n",
    "**Operation**: $f(x) = \\max(0, x)$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_scalar = F.relu(a)\n",
    "res_scalar.backward()\n",
    "check_result_and_grads(res_scalar, a, operation=\"relu\", itype=\"scalar\")\n",
    "\n",
    "res_array = F.relu(C)\n",
    "res_array[0,0].backward()\n",
    "check_result_and_grads(res_array, C, operation=\"relu\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "***The derivative of the softmax is not trivial to computein a vectorized manner, I have done the exercice and give you my implementation of the softmax, feel free to ask me questions about it.***\n",
    "\n",
    "***You have to fill bellow the formulas!***\n",
    "\n",
    "**Inputs**: $x$\n",
    "\n",
    "**Operation**: $f(x) = \\frac{e^{x_i}}{\\sum_i e^{x_i}}$\n",
    "\n",
    "**Derivatives**:\n",
    "+ **w.r.t.** $x$: \n",
    "    \n",
    "    $\\frac{\\partial f}{\\partial x} = ...$ *Fill here*\n",
    "    \n",
    "    By chain rule:\n",
    "    $\\frac{\\partial}{\\partial x} = \\frac{\\partial}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x} = ...$ *Fill here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_variables(a, b, C, D)\n",
    "\n",
    "res_array = F.softmax(C, dim=0)\n",
    "res_array[0,0].backward()\n",
    "\n",
    "check_result_and_grads(res_array, C, operation=\"softmax\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss\n",
    "\n",
    "For the Cross entropy loss we have used  the trick from pytorch that implements direclty the cross entropy loss with the softmax for more stability.\n",
    "\n",
    "Take a look [here](https://pytorch.org/docs/stable/nn.html#crossentropyloss).\n",
    "\n",
    "You don't have to implement it but make sure you understand what append here, comment it on your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn as nn\n",
    "\n",
    "X = Variable([[0.1711, 0.5140, 0.3149], [0.1359, 0.4985, 0.3656], [0.0275, 0.5467, 0.4258]])\n",
    "y = Variable([1, 2, 0])\n",
    "\n",
    "cel = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = cel(X, y)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "check_result_and_grads(loss, X, operation=\"CEL\", itype=\"array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An MLP as example\n",
    "\n",
    "Now that you have all the components filled for the graph computational, you will need some additional steps to make a MLP trainable.\n",
    "\n",
    "You have to complete the missing parts in ***nn.py*** and in ***optim.py***.\n",
    "\n",
    "\n",
    "First we will generate a simple dataset, each color represents each class.\n",
    "As you can see, we have 3 classes and each sample has 2 features (the cordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "# Generate a dataset and plot it# Gener\n",
    "N = 500\n",
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_blobs(N)\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y)\n",
    "\n",
    "X_train = X[:350]\n",
    "y_train = y[:350]\n",
    "X_test = X[350:]\n",
    "y_test = y[350:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the MLP\n",
    "\n",
    "Follow the todos here and complete the missing parts in ***nn.py*** and ***optim.py***.\n",
    "\n",
    "List of things you have to do. You can put a 'x' inside the [ ] when you have done it!\n",
    "Example: * [x] Example done.\n",
    "\n",
    "\n",
    "\n",
    "* nn.py:\n",
    "    * Linear:\n",
    "        * in init\n",
    "            * [ ] Initialize the weights.\n",
    "            * [ ] Initialize the bias.\n",
    "        * in call:\n",
    "            * [ ] Implement the linear transformation.\n",
    "            * [ ] Add the bias.\n",
    "* optim.py:\n",
    "    * SGD:\n",
    "        * in step:\n",
    "            * [ ] Implement the SGD update mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional import F\n",
    "from variable import Variable\n",
    "import nn as nn\n",
    "from optim import SGD\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, out_features):\n",
    "        #######################################################################\n",
    "        # TODO: define 2 linear layers, one that takes the inputs and outputs\n",
    "        # values with hidden_size\n",
    "        # and the second one that takes the values from the first layer and\n",
    "        # outputs the scores. \n",
    "        # implement Linear in nn.py before, you need it here.\n",
    "        #######################################################################\n",
    "        pass\n",
    "        #######################################################################\n",
    "        # --------------------------- END OF YOUR CODE ------------------------\n",
    "        #######################################################################\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output = None\n",
    "        #######################################################################\n",
    "        # TODO: define your forward pass as follow\n",
    "        #    1) y = linear(inputs)\n",
    "        #    2) y_nl = relu(y)\n",
    "        #    3) output = linear(y_nl)\n",
    "        # softmax not needed because it's already in cross entropy\n",
    "        #######################################################################\n",
    "        pass\n",
    "        #######################################################################\n",
    "        # --------------------------- END OF YOUR CODE ------------------------\n",
    "        #######################################################################\n",
    "        return output\n",
    "\n",
    "\n",
    "model = MLP(2, 100, 3)\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 50\n",
    "\n",
    "history_losses = []\n",
    "history_acc = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    \n",
    "    indices = range(X_train.shape[0])\n",
    "\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    \n",
    "    for iteration in range(X_train.shape[0]//batch_size):\n",
    "        batch_indices = np.random.choice(indices, batch_size)\n",
    "        indices = list(set(indices) - set(batch_indices))\n",
    "\n",
    "        X_batch = Variable(X_train[batch_indices])\n",
    "        y_batch = Variable(y_train[batch_indices])\n",
    "        \n",
    "        \n",
    "        #######################################################################\n",
    "        # TODO: Add here all the elements you need to train your model for each\n",
    "        # batch.\n",
    "        #######################################################################\n",
    "        \n",
    "        # you need to clear out the gradients for all the parameters\n",
    "        pass\n",
    "        \n",
    "        # compute the forward pass\n",
    "        pass\n",
    "        \n",
    "        # compute tht loss\n",
    "        pass\n",
    "        \n",
    "        # compute the backward pass\n",
    "        pass\n",
    "        \n",
    "        # optimize\n",
    "        pass\n",
    "        #######################################################################\n",
    "        # --------------------------- END OF YOUR CODE ------------------------\n",
    "        #######################################################################\n",
    "\n",
    "        # keep loss\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # keep accuracy\n",
    "        y_pred = np.argmax(outputs.data, axis=1)\n",
    "        train_acc.append((y_pred[:, None] == y_batch.data).mean())\n",
    "    \n",
    "    history_losses.append(np.mean(train_losses))\n",
    "    history_acc.append(np.mean(train_acc))\n",
    "    \n",
    "    # mod allow us to only display in a logaritmic way\n",
    "    mod = 10**np.floor(np.log10(epoch))\n",
    "    if epoch % mod == 0:\n",
    "        print(\"Epoch {:>3}/{:>3}, loss {:.4f}, acc {:.2f}\".format(epoch, epochs, history_losses[-1], history_acc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "Now you can visualise for fun the loss and the accuracy of your model during trainning and get the final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_losses, c=\"r\", label=\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_acc, c=\"g\", label=\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get ~90% of accuracy with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "X_test_var = Variable(X_test)\n",
    "\n",
    "outputs = model(X_test_var)\n",
    "\n",
    "y_pred = np.argmax(outputs.data, axis=1)\n",
    "acc = (y_pred == y_test).mean()\n",
    "\n",
    "print(\"Accuracy on test set: {:.2f}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
